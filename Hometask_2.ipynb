{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e84265b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pymystem3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d849c47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install natasha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a2547b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793db2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m spacy download ru_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaef595d",
   "metadata": {},
   "source": [
    "## Домашнее задание номер 2\n",
    "\n",
    "На последнем семинаре мы проанализировали несколько различных морфологических теггеров. Как же узнать, какой использовать? Давайте сравним их качество!\n",
    "\n",
    "В этой домашке вам будет нужно найти тексты на русском языке (размер корпуса не менее 200 слов), \n",
    "в которых  будут какие-то трудные или неоднозначные для POS теггинга моменты и разметить их вручную \n",
    "– с помощью этих текстов мы будем оценивать качество работы наших теггеров. В текстах размечаем только части речи, ничего больше!\n",
    "1. (1 балл) Создание, разметка корпуса и объяснение того, почему этот текст подходит для оценки (какие моменты вы тут считаете трудными для автоматического посттеггинга и почему, в этом вам может помочь второй ридинг). Не забывайте, что разные теггеры могут использовать разные тегсеты: напишите комментарий о том, какой тегсет вы берёте для разметки и почему."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6ab73d",
   "metadata": {},
   "source": [
    "Текст в файле **sents.txt**, разметка текста в файле **gold_standard.txt**. \n",
    "\n",
    "Трудные для постеггинга моменты в моём тексте:\n",
    "- аббревиатуры *ООП*, *ВПО*, *ФГОАУ* и т.п. -- их можно не распознать совсем или спутать с другими частями речи, например, при соответствующих окончаниях\n",
    "- употребительные сокращения *ст. научн. сотр.* и т.п. -- этих сокращений вполне вероятно нет в словарях моделей и, особенно когда за ними скрываются не существительные, это будет трудный случай для моделей\n",
    "- составной предлог *за счет*, который я решила делить на две части в угоду орфографии, но помечать обе части как предлог.\n",
    "- конверсия *родные*, *исполняющий* в И. о., *обратное* в *в обратном*, *данные*\n",
    "- слова через дефис типа *день-другой*, *ну-ка*, *ходить-бродить* - возникает в первую очередь вопрос, как их делить на токены, а затем, соответственной, какую часть речи приписывать (результат будет различным в зависимости от результатов токенизации)\n",
    "- предложение с переставленными буквами внутри слова -- человек может понимать такие предложения, поэтому хотелось бы, чтобы и модели могли\n",
    "- омоформы вроде *стекла*, *мечи*, *косой*, *коса*, *кося* и пр., которые, очевидно, можно перепутать\n",
    "- неочевидные демонстративы *прочий*, *некоторый*\n",
    "- слова, которых скорее всего нет в словарях у использующихся моделей *моджибаке*, *памимимный*, *ведомостичка* и т.п.\n",
    "- нелитературные формы слов *побежду*, *убедю* -- это формы глаголов, которых наверняка нет в словарях моделей, а мы кроме того знаем из ридинга, что глаголы модели предсказывают с меньшей вероятностью, чем существительные и т.п.\n",
    "\n",
    "Для разметки я взяла тегсет, основанный на стандарте UD, это удобно, так как этот стандарт на данный момент считается наиболее универсальными. Тем не менее, я внесла некоторые изменения и упрощения, в целом из неочевидного:\n",
    "- соответствующие имена собственные считаются существительными (позволит верно соотнести категории из UD и из mystem, который, соответственно, считает имена собственные просто существительными)\n",
    "- вспомогательные глаголы типа *быть* считаются глаголами (это более характерно для русской традиции и позволит избежать сложностей при работе с тегами, более ориентированными на русский язык как у mystem)\n",
    "- все виды причастий и деепричастий -- также формы глаголов (это удобно, потому что сокращает количество частеречных тегов, кроме того причастия и деепричастия однозначно образовываются от глаголов и невыделение их в отдельные классы более универсально с типологической точки зрения)\n",
    "- *прочий*, *некоторый*, *один* (в соотв. контексте) -- детерминативы, так как это логично для стандарта UD и в стандарте mystem тоже находит соответствие -- категорию APRO, объединяющую местоимённые прилагательные. Тогда в категории местоимений остаются только местоимённые существительные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95bdcf72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Финансовое обеспечение деятельности Федерального государственного автономного образовательного учреждения высшего профессионального образования ФГОАУ ВПО Национального исследовательского университета «Высшая школа экономики» (далее НИУ ВШЭ) за счет средств федерального бюджета осуществляет Управление делами Президента Российской Федерации.',\n",
       " 'К 2018 году 100% ООП ВПО прошли экспертизу с участием международных экспертов.',\n",
       " 'Под Марксом, в кресло вкресленный с высоким окладом, высок и гладок, сидит облачённый ответственный.',\n",
       " 'Каждый на месте: невеста — в тресте, кум — в Гум, брат — в наркомат.',\n",
       " 'Все шире периферия родных, и в ведомостичках узких не вместишь всех сортов наградных — спецставки, тантьемы, нагрузки!']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"sents.txt\", 'r', encoding='utf-8') as file:\n",
    "    sents = file.read()\n",
    "\n",
    "sents = sents.split('\\n')\n",
    "sents[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ee1d0d",
   "metadata": {},
   "source": [
    "Создаём список списков токенов и POS-тегов на основе образцовой разметки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "607720ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"gold_standard.txt\", 'r', encoding='utf-8') as file:\n",
    "    tokens = file.read()\n",
    "    \n",
    "tokens = tokens.split('\\n')\n",
    "gold_standard_list = []\n",
    "for token in tokens:\n",
    "    gold_standard_list.append(token.split('\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcf7ba12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Финансовое', 'ADJ'],\n",
       " ['обеспечение', 'NOUN'],\n",
       " ['деятельности', 'NOUN'],\n",
       " ['Федерального', 'ADJ'],\n",
       " ['государственного', 'ADJ'],\n",
       " ['автономного', 'ADJ'],\n",
       " ['образовательного', 'ADJ'],\n",
       " ['учреждения', 'NOUN'],\n",
       " ['высшего', 'ADJ'],\n",
       " ['профессионального', 'ADJ']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_standard_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c25a91",
   "metadata": {},
   "source": [
    "2. (3 балла) Потом вам будет нужно взять три  POS теггера для русского языка (udpipe, stanza, natasha, pymorphy, mystem, spacy, deeppavlov) и «прогнать» текст через каждый из них."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affb8561",
   "metadata": {},
   "source": [
    "### Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0311448",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f00c0d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Mystem()\n",
    "mystem_analysis_list = []\n",
    "for sent in sents:\n",
    "    sent_analysis = m.analyze(sent)\n",
    "    for i in range(len(sent_analysis)):\n",
    "        # Исключаем из анализа пробелы и пунктуацию\n",
    "        if 'analysis' in sent_analysis[i]:\n",
    "            lexeme = sent_analysis[i]['text']\n",
    "            # Вписываем отдельно слова, для которых анализатор не смог определить часть речи\n",
    "            if sent_analysis[i]['analysis'] != []:\n",
    "                morph = sent_analysis[i]['analysis'][0]['gr']\n",
    "                pos = morph.split('=')[0].split(',')[0]\n",
    "            else:\n",
    "                pos = None\n",
    "            mystem_analysis_list.append([lexeme, pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3fbf6bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Финансовое', 'A'],\n",
       " ['обеспечение', 'S'],\n",
       " ['деятельности', 'S'],\n",
       " ['Федерального', 'A'],\n",
       " ['государственного', 'A']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystem_analysis_list[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5f81e9",
   "metadata": {},
   "source": [
    "### Natasha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed6ffb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import Segmenter, NewsEmbedding, NewsMorphTagger, Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74ba8dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmenter = Segmenter()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eda08c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "natasha_analysis = Doc(\" \".join(sents))\n",
    "natasha_analysis.segment(segmenter)\n",
    "natasha_analysis.tag_morph(morph_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8de29b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MorphToken(\n",
       "    text='Финансовое',\n",
       "    pos='ADJ',\n",
       "    feats={'Case': 'Nom',\n",
       "     'Degree': 'Pos',\n",
       "     'Gender': 'Neut',\n",
       "     'Number': 'Sing'}\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "natasha_analysis.sents[0].morph.tokens[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f35eeb2",
   "metadata": {},
   "source": [
    "### Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "645a04b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c47179c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"ru_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "949fb750",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_doc = nlp(\" \".join(sents))\n",
    "spacy_analysis_list = []\n",
    "for token in spacy_doc:\n",
    "    spacy_analysis_list.append([token.text, token.pos_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a443f209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Финансовое', 'ADJ'],\n",
       " ['обеспечение', 'NOUN'],\n",
       " ['деятельности', 'NOUN'],\n",
       " ['Федерального', 'ADJ'],\n",
       " ['государственного', 'ADJ'],\n",
       " ['автономного', 'ADJ'],\n",
       " ['образовательного', 'ADJ'],\n",
       " ['учреждения', 'NOUN'],\n",
       " ['высшего', 'ADJ'],\n",
       " ['профессионального', 'ADJ']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_analysis_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d48aa9",
   "metadata": {},
   "source": [
    "3. (2 балла) Затем оценим accuracy для каждого теггера. Заметьте, что в разных системах имена тегов и части речи  могут отличаться, – вам надо будет свести это всё к единому стандарту с помощью какой-то функции-конвертера и сравнить с вашим размеченным руками эталоном - тоже с помощью какого-то кода или функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9610732f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tags(word_pos_tag, model):\n",
    "    if model.lower() == 'mystem':\n",
    "        dict_corr = {'A': 'ADJ', 'PR': 'ADP', 'S': 'NOUN', 'V': 'VERB', 'ANUM': 'NUM',\n",
    "                                      'APRO': 'DET', 'ADVPRO': 'ADV', 'SPRO': 'PRO'}\n",
    "    elif model.lower() == 'natasha' or model.lower() == 'spacy':\n",
    "        dict_corr = {'PROPN': 'NOUN', 'AUX': 'VERB', 'CCONJ': 'CONJ', 'SCONJ': 'CONJ', 'PUNCT': None}\n",
    "    \n",
    "    if word_pos_tag in dict_corr:\n",
    "        return(dict_corr[word_pos_tag])\n",
    "    else:\n",
    "        return(word_pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "838a96d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mystem_conv_analysis_list = []\n",
    "for mystem_word_analysis in mystem_analysis_list:\n",
    "    mystem_conv_tag = convert_tags(mystem_word_analysis[1], 'mystem')\n",
    "    mystem_conv_analysis_list.append([mystem_word_analysis[0], mystem_conv_tag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "707e6dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Финансовое', 'ADJ'],\n",
       " ['обеспечение', 'NOUN'],\n",
       " ['деятельности', 'NOUN'],\n",
       " ['Федерального', 'ADJ'],\n",
       " ['государственного', 'ADJ']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystem_conv_analysis_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3dce81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "natasha_conv_analysis_list = []\n",
    "for natasha_sent_analysis in natasha_analysis.sents:\n",
    "    for natasha_word_analysis in natasha_sent_analysis.morph.tokens:\n",
    "        natasha_conv_word_analysis = [natasha_word_analysis.text,\n",
    "                                           convert_tags(natasha_word_analysis.pos, 'natasha')]\n",
    "        if natasha_conv_word_analysis[1] != None:\n",
    "            natasha_conv_analysis_list.append(natasha_conv_word_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab9651d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Финансовое', 'ADJ'],\n",
       " ['обеспечение', 'NOUN'],\n",
       " ['деятельности', 'NOUN'],\n",
       " ['Федерального', 'ADJ'],\n",
       " ['государственного', 'ADJ']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "natasha_conv_analysis_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f58d9e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_conv_analysis_list = []\n",
    "for spacy_word_analysis in spacy_analysis_list:\n",
    "    spacy_conv_tag = convert_tags(spacy_word_analysis[1], 'spacy')\n",
    "    if spacy_conv_tag != None:\n",
    "        spacy_conv_analysis_list.append([spacy_word_analysis[0], spacy_conv_tag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7a660b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Финансовое', 'ADJ'],\n",
       " ['обеспечение', 'NOUN'],\n",
       " ['деятельности', 'NOUN'],\n",
       " ['Федерального', 'ADJ'],\n",
       " ['государственного', 'ADJ'],\n",
       " ['автономного', 'ADJ'],\n",
       " ['образовательного', 'ADJ'],\n",
       " ['учреждения', 'NOUN'],\n",
       " ['высшего', 'ADJ'],\n",
       " ['профессионального', 'ADJ']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_conv_analysis_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eda3a26",
   "metadata": {},
   "source": [
    "При сопоставлении самое сложное -- разобраться со съехавшими токенами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc6bf575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(231, 231, 232, 240)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gold_standard_list), len(mystem_conv_analysis_list), len(natasha_conv_analysis_list), len(spacy_conv_analysis_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3634cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv   \n",
    "    \n",
    "with open('analysis_comparison.tsv', 'w', newline='', encoding='utf-8') as file:\n",
    "    longest_analysis_len = max(len(gold_standard_list), len(mystem_conv_analysis_list),\n",
    "                               len(natasha_conv_analysis_list), len(spacy_conv_analysis_list))\n",
    "    for i in range(longest_analysis_len):\n",
    "        if len(gold_standard_list) <= i:\n",
    "            gold_standard_analysis = ['', '']\n",
    "        else:\n",
    "            gold_standard_analysis = gold_standard_list[i]\n",
    "        if len(mystem_conv_analysis_list) <= i:\n",
    "            mystem_analysis = ['', '']\n",
    "        else:\n",
    "            mystem_analysis = mystem_conv_analysis_list[i]\n",
    "        if len(natasha_conv_analysis_list) <= i:\n",
    "            natasha_analysis = ['', '']\n",
    "        else:\n",
    "            natasha_analysis = natasha_conv_analysis_list[i]\n",
    "        spacy_analysis = spacy_conv_analysis_list[i]\n",
    "        string = [gold_standard_analysis[0], gold_standard_analysis[1],\n",
    "                  mystem_analysis[0], mystem_analysis[1],\n",
    "                  natasha_analysis[0], natasha_analysis[1],\n",
    "                  spacy_analysis[0], spacy_analysis[1]]\n",
    "        tsv_output = csv.writer(file, delimiter='\\t')\n",
    "        tsv_output.writerow(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3205c7",
   "metadata": {},
   "source": [
    "Далее наполовину вручную разбираемся с произошедшими из-за различных алгоритмов токенизации у разных моделей сдвигами в соответствии токенов. Если морфологизатор неверно делил слово на токены, то брался pos-тег последнего токена."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7d10f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "38d6b5f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gold_standard_token</th>\n",
       "      <th>gold_standard_tag</th>\n",
       "      <th>mystem_tag</th>\n",
       "      <th>natasha_tag</th>\n",
       "      <th>spacy_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>косого</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>VERB</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>косо</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>косил</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>косую</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>косу</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    gold_standard_token gold_standard_tag mystem_tag natasha_tag spacy_tag\n",
       "226              косого               ADJ        ADJ        VERB       ADJ\n",
       "227                косо               ADV        ADV         ADV       ADV\n",
       "228               косил              VERB       VERB        VERB      VERB\n",
       "229               косую               ADJ        ADJ         ADJ      NOUN\n",
       "230                косу              NOUN       NOUN        NOUN      NOUN"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('analysis_comparison.tsv', sep='\\t', encoding='utf-8', header=None)\n",
    "df.columns = ['gold_standard_token', 'gold_standard_tag',\n",
    "              'mystem_token', 'mystem_tag',\n",
    "              'natasha_token', 'natasha_tag',\n",
    "              'spacy_token', 'spacy_tag']\n",
    "df.loc[33:,['mystem_token', 'mystem_tag']] = df.loc[33:,['mystem_token', 'mystem_tag']].shift(1)\n",
    "df.loc[35:,['mystem_token', 'mystem_tag']] = df.loc[35:,['mystem_token', 'mystem_tag']].shift(1)\n",
    "df.loc[36:,['natasha_token', 'natasha_tag', 'spacy_token', 'spacy_tag']] = \\\n",
    "                df.loc[36:,['natasha_token', 'natasha_tag', 'spacy_token', 'spacy_tag']].shift(-1)\n",
    "df.loc[89:,['mystem_token', 'mystem_tag']] = df.loc[89:,['mystem_token', 'mystem_tag']].shift(-1)\n",
    "df.loc[89:,['spacy_token', 'spacy_tag']] = df.loc[89:,['spacy_token', 'spacy_tag']].shift(-2)\n",
    "df.loc[118:,['spacy_token', 'spacy_tag']] = df.loc[118:,['spacy_token', 'spacy_tag']].shift(-1)\n",
    "df.loc[149:,['spacy_token', 'spacy_tag']] = df.loc[149:,['spacy_token', 'spacy_tag']].shift(-1)\n",
    "df.loc[150:,['spacy_token', 'spacy_tag']] = df.loc[150:,['spacy_token', 'spacy_tag']].shift(-2)\n",
    "df.loc[165:,['mystem_token', 'mystem_tag']] = df.loc[165:,['mystem_token', 'mystem_tag']].shift(-1)\n",
    "df.loc[165:,['spacy_token', 'spacy_tag']] = df.loc[165:,['spacy_token', 'spacy_tag']].shift(-2)\n",
    "df = df.loc[:230, ['gold_standard_token', 'gold_standard_tag', 'mystem_tag', 'natasha_tag', 'spacy_tag']]\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8516398c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff07593a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mystem 0.8051948051948052\n",
      "Natasha 0.8398268398268398\n",
      "Spacy 0.8311688311688312\n"
     ]
    }
   ],
   "source": [
    "print('Mystem', accuracy_score(df['gold_standard_tag'], df['mystem_tag'].astype(str)))\n",
    "print('Natasha', accuracy_score(df['gold_standard_tag'], df['natasha_tag']))\n",
    "print('Spacy', accuracy_score(df['gold_standard_tag'], df['spacy_tag']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f125bd",
   "metadata": {},
   "source": [
    "Итак, из тех постеггеров, которые я сравнивала, лучшим оказался Natasha."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22385efe",
   "metadata": {},
   "source": [
    "4. (4 балла) Дальше вам нужно взять лучший теггер для русского языка и с его помощью написать функцию (chunker),  которая выделяет из размеченного текста 3 типа n-грамм, соответствующих какому-то шаблону (к примеру не + какая-то часть речи или NP или сущ.+ наречие и тд) В предыдущем дз многие из вас справедливо заметили, что если бы мы могли класть в словарь не только отдельные слова, но и словосочетания, то программа работала бы лучше. Предложите 3 шаблона (слово + POS-тег / POS-тег + POS-тег) запись которых в словарь, по вашему мнению, улучшила бы качество работы программы из предыдущей домашки. Балл за объяснение того, почему именно эти группы вы взяли, балл за создание такого рода чанкера, балл за  за встраивание функции в программу из предыдущей домашки, балл за сравнение качества предсказания тональности с улучшением и без."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607039ec",
   "metadata": {},
   "source": [
    "Поизучав тексты отзывов к фильмам, которые я использовала, я решила выбрать следующие шаблоны:\n",
    "- *не* + VERB\n",
    "- *слишком* + ADV/ADJ\n",
    "- *хороший* + NOUN\n",
    "\n",
    "Сочетания глагола с *не* часто используются, чтобы описать, что не понравилось в фильме, ср. *не тянут, не понравился, не рекомендую* и т.п. Сочетания со *слишком* тоже несут отрицательные оттенки значения из-за семантики этого наречия, довольно часто пишут: *слишком много, уныло, плоский* и т.п. *Хороший* + существительное же, наоборот, направлено на выявление положительных отзывов. Довольно часто пишут *хороший фильм, хорошая документалка* и т.п., когда говорят о положительных эмоциях от фильма, при этом просто *хороший* без дополнительных условий на существительное после него допускает чуть больше вариативности в том, что это за словосочетания, ср. *ни к чему хорошему не приведёт* в отрицательном отзыве."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e5d05403",
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import MorphVocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "acd6705e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker(text):\n",
    "    \n",
    "    segmenter = Segmenter()\n",
    "    emb = NewsEmbedding()\n",
    "    morph_tagger = NewsMorphTagger(emb)\n",
    "    morph_vocab = MorphVocab()\n",
    "\n",
    "    tagged_text = Doc(text)\n",
    "    tagged_text.segment(segmenter)\n",
    "    tagged_text.tag_morph(morph_tagger)\n",
    "\n",
    "    tagged_list = []\n",
    "    for tagged_word in tagged_text.tokens:\n",
    "        tagged_word.lemmatize(morph_vocab)\n",
    "        tagged_conv_words = [tagged_word.lemma, convert_tags(tagged_word.pos, 'natasha')]\n",
    "        if tagged_conv_words[1] != None:\n",
    "            tagged_list.append(tagged_conv_words)\n",
    "    \n",
    "    df = pd.DataFrame(tagged_list)\n",
    "    df.columns = ['lemma', 'tag']\n",
    "    df['next_lemma'] = df['lemma'].shift(-1)\n",
    "    df['next_tag'] = df['tag'].shift(-1)\n",
    "    \n",
    "    return df[(df['lemma'] == 'не') & (df['next_tag'] == 'VERB')], \\\n",
    "        df[(df['lemma'] == 'слишком') & ((df['next_tag'] == 'ADV') | (df['next_tag'] == 'ADJ'))], \\\n",
    "        df[(df['lemma'] == 'хороший') & (df['next_tag'] == 'NOUN')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3d2271f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma</th>\n",
       "      <th>tag</th>\n",
       "      <th>next_lemma</th>\n",
       "      <th>next_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>не</td>\n",
       "      <td>PART</td>\n",
       "      <td>вместить</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>не</td>\n",
       "      <td>PART</td>\n",
       "      <td>дождаться</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>не</td>\n",
       "      <td>PART</td>\n",
       "      <td>иеемт</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>не</td>\n",
       "      <td>PART</td>\n",
       "      <td>хотеть</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>не</td>\n",
       "      <td>PART</td>\n",
       "      <td>бывать</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>не</td>\n",
       "      <td>PART</td>\n",
       "      <td>бывать</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    lemma   tag next_lemma next_tag\n",
       "80     не  PART   вместить     VERB\n",
       "94     не  PART  дождаться     VERB\n",
       "127    не  PART      иеемт     VERB\n",
       "161    не  PART     хотеть     VERB\n",
       "173    не  PART     бывать     VERB\n",
       "180    не  PART     бывать     VERB"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunker(\" \".join(sents))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6578965a",
   "metadata": {},
   "source": [
    "Встроить функцию в программу из предыдущей домашки я решила в ноутбуке предыдущей домашки в новом последнем разделе -- файл Hometask_1_better. Там же можно увидеть сравнение качества предсказания тональности с улучшением и без. Конечно, многое зависит от случайности, так как при таком количестве тестирующих примеров accuracy не является сильно точным значением, однако на последнем моём тесте это улучшение действительно улучшило качество: с 0.65 до 0.7."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
