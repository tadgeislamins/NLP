{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e84265b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pymystem3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d849c47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install natasha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a2547b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793db2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m spacy download ru_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaef595d",
   "metadata": {},
   "source": [
    "## Домашнее задание номер 2\n",
    "\n",
    "На последнем семинаре мы проанализировали несколько различных морфологических теггеров. Как же узнать, какой использовать? Давайте сравним их качество!\n",
    "\n",
    "В этой домашке вам будет нужно найти тексты на русском языке (размер корпуса не менее 200 слов), \n",
    "в которых  будут какие-то трудные или неоднозначные для POS теггинга моменты и разметить их вручную \n",
    "– с помощью этих текстов мы будем оценивать качество работы наших теггеров. В текстах размечаем только части речи, ничего больше!\n",
    "1. (1 балл) Создание, разметка корпуса и объяснение того, почему этот текст подходит для оценки (какие моменты вы тут считаете трудными для автоматического посттеггинга и почему, в этом вам может помочь второй ридинг). Не забывайте, что разные теггеры могут использовать разные тегсеты: напишите комментарий о том, какой тегсет вы берёте для разметки и почему."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6ab73d",
   "metadata": {},
   "source": [
    "Текст в файле **sents.txt**, разметка текста в файле **gold_standard.txt**. \n",
    "\n",
    "Трудные для постеггинга моменты в моём тексте:\n",
    "- аббревиатуры *ООП*, *ВПО*, *ФГОАУ* и т.п. -- их можно не распознать совсем или спутать с другими частями речи, например, при соответствующих окончаниях\n",
    "- употребительные сокращения *ст. научн. сотр.* и т.п. -- этих сокращений вполне вероятно нет в словарях моделей и, особенно когда за ними скрываются не существительные, это будет трудный случай для моделей\n",
    "- составной предлог *за счет*, который я решила делить на две части в угоду орфографии, но помечать обе части как предлог.\n",
    "- конверсия *родные*, *исполняющий* в И. о., *обратное* в *в обратном*, *данные*\n",
    "- слова через дефис типа *день-другой*, *ну-ка*, *ходить-бродить* - возникает в первую очередь вопрос, как их делить на токены, а затем, соответственной, какую часть речи приписывать (результат будет различным в зависимости от результатов токенизации)\n",
    "- предложение с переставленными буквами внутри слова -- человек может понимать такие предложения, поэтому хотелось бы, чтобы и модели могли\n",
    "- омоформы вроде *стекла*, *мечи*, *косой*, *коса*, *кося* и пр., которые, очевидно, можно перепутать\n",
    "- неочевидные демонстративы *прочий*, *некоторый*\n",
    "- слова, которых скорее всего нет в словарях у использующихся моделей *моджибаке*, *памимимный*, *ведомостичка* и т.п.\n",
    "- нелитературные формы слов *побежду*, *убедю* -- это формы глаголов, которых наверняка нет в словарях моделей, а мы кроме того знаем из ридинга, что глаголы модели предсказывают с меньшей вероятностью, чем существительные и т.п.\n",
    "\n",
    "Для разметки я взяла тегсет, основанный на стандарте UD, это удобно, так как этот стандарт на данный момент считается наиболее универсальными. Тем не менее, я внесла некоторые изменения и упрощения, в целом из неочевидного:\n",
    "- соответствующие имена собственные считаются существительными (позволит верно соотнести категории из UD и из mystem, который, соответственно, считает имена собственные просто существительными)\n",
    "- вспомогательные глаголы типа *быть* считаются глаголами (это более характерно для русской традиции и позволит избежать сложностей при работе с тегами, более ориентированными на русский язык как у mystem)\n",
    "- все виды причастий и деепричастий -- также формы глаголов (это удобно, потому что сокращает количество частеречных тегов, кроме того причастия и деепричастия однозначно образовываются от глаголов и невыделение их в отдельные классы более универсально с типологической точки зрения)\n",
    "- *прочий*, *некоторый*, *один* (в соотв. контексте) -- детерминативы, так как это логично для стандарта UD и в стандарте mystem тоже находит соответствие -- категорию APRO, объединяющую местоимённые прилагательные. Тогда в категории местоимений остаются только местоимённые существительные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "95bdcf72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Финансовое обеспечение деятельности Федерального государственного автономного образовательного учреждения высшего профессионального образования ФГОАУ ВПО Национального исследовательского университета «Высшая школа экономики» (далее НИУ ВШЭ) за счет средств федерального бюджета осуществляет Управление делами Президента Российской Федерации.',\n",
       " 'К 2018 году 100% ООП ВПО прошли экспертизу с участием международных экспертов.',\n",
       " 'Под Марксом, в кресло вкресленный с высоким окладом, высок и гладок, сидит облачённый ответственный.',\n",
       " 'Каждый на месте: невеста — в тресте, кум — в Гум, брат — в наркомат.',\n",
       " 'Все шире периферия родных, и в ведомостичках узких не вместишь всех сортов наградных — спецставки, тантьемы, нагрузки!']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"sents.txt\", 'r', encoding='utf-8') as file:\n",
    "    sents = file.read()\n",
    "\n",
    "sents = sents.split('\\n')\n",
    "sents[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47acd10f",
   "metadata": {},
   "source": [
    "Создаём список списков токенов и POS-тегов на основе образцовой разметки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "607720ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"gold_standard.txt\", 'r', encoding='utf-8') as file:\n",
    "    tokens = file.read()\n",
    "    \n",
    "tokens = tokens.split('\\n')\n",
    "gold_standard_list = []\n",
    "for token in tokens:\n",
    "    gold_standard_list.append(token.split('\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "38c4b2e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Финансовое', 'ADJ'],\n",
       " ['обеспечение', 'NOUN'],\n",
       " ['деятельности', 'NOUN'],\n",
       " ['Федерального', 'ADJ'],\n",
       " ['государственного', 'ADJ'],\n",
       " ['автономного', 'ADJ'],\n",
       " ['образовательного', 'ADJ'],\n",
       " ['учреждения', 'NOUN'],\n",
       " ['высшего', 'ADJ'],\n",
       " ['профессионального', 'ADJ']]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_standard_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c25a91",
   "metadata": {},
   "source": [
    "2. (3 балла) Потом вам будет нужно взять три  POS теггера для русского языка (udpipe, stanza, natasha, pymorphy, mystem, spacy, deeppavlov) и «прогнать» текст через каждый из них."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affb8561",
   "metadata": {},
   "source": [
    "### Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0311448",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f00c0d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Mystem()\n",
    "mystem_analysis_list = []\n",
    "for sent in sents:\n",
    "    sent_analysis = m.analyze(sent)\n",
    "    for i in range(len(sent_analysis)):\n",
    "        # Исключаем из анализа пробелы и пунктуацию\n",
    "        if 'analysis' in sent_analysis[i]:\n",
    "            lexeme = sent_analysis[i]['text']\n",
    "            # Вписываем отдельно слова, для которых анализатор не смог определить часть речи\n",
    "            if sent_analysis[i]['analysis'] != []:\n",
    "                morph = sent_analysis[i]['analysis'][0]['gr']\n",
    "                pos = morph.split('=')[0].split(',')[0]\n",
    "            else:\n",
    "                pos = None\n",
    "            mystem_analysis_list.append([lexeme, pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "13e54539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Финансовое', 'A'],\n",
       " ['обеспечение', 'S'],\n",
       " ['деятельности', 'S'],\n",
       " ['Федерального', 'A'],\n",
       " ['государственного', 'A']]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystem_analysis_list[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5f81e9",
   "metadata": {},
   "source": [
    "### Natasha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ed6ffb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import Segmenter, NewsEmbedding, NewsMorphTagger, Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "74ba8dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmenter = Segmenter()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "eda08c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "natasha_analysis = Doc(\" \".join(sents))\n",
    "natasha_analysis.segment(segmenter)\n",
    "natasha_analysis.tag_morph(morph_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e8de29b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MorphToken(\n",
       "    text='Финансовое',\n",
       "    pos='ADJ',\n",
       "    feats={'Case': 'Nom',\n",
       "     'Degree': 'Pos',\n",
       "     'Gender': 'Neut',\n",
       "     'Number': 'Sing'}\n",
       ")"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "natasha_analysis.sents[0].morph.tokens[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f35eeb2",
   "metadata": {},
   "source": [
    "### Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "645a04b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5c47179c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"ru_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "949fb750",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_doc = nlp(\" \".join(sents))\n",
    "spacy_analysis_list = []\n",
    "for token in spacy_doc:\n",
    "    spacy_analysis_list.append([token.text, token.pos_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "a5a6e8a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Финансовое', 'ADJ'],\n",
       " ['обеспечение', 'NOUN'],\n",
       " ['деятельности', 'NOUN'],\n",
       " ['Федерального', 'ADJ'],\n",
       " ['государственного', 'ADJ'],\n",
       " ['автономного', 'ADJ'],\n",
       " ['образовательного', 'ADJ'],\n",
       " ['учреждения', 'NOUN'],\n",
       " ['высшего', 'ADJ'],\n",
       " ['профессионального', 'ADJ']]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_analysis_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d48aa9",
   "metadata": {},
   "source": [
    "3. (2 балла) Затем оценим accuracy для каждого теггера. Заметьте, что в разных системах имена тегов и части речи  могут отличаться, – вам надо будет свести это всё к единому стандарту с помощью какой-то функции-конвертера и сравнить с вашим размеченным руками эталоном - тоже с помощью какого-то кода или функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "9610732f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tags(word_pos_tag, model):\n",
    "    if model.lower() == 'mystem':\n",
    "        dict_corr = {'A': 'ADJ', 'PR': 'ADP', 'S': 'NOUN', 'V': 'VERB', 'ANUM': 'NUM',\n",
    "                                      'APRO': 'DET', 'ADVPRO': 'ADV', 'SPRO': 'PRO'}\n",
    "    elif model.lower() == 'natasha' or model.lower() == 'spacy':\n",
    "        dict_corr = {'PROPN': 'NOUN', 'AUX': 'VERB', 'CCONJ': 'CONJ', 'SCONJ': 'CONJ', 'PUNCT': None}\n",
    "    \n",
    "    if word_pos_tag in dict_corr:\n",
    "        return(dict_corr[word_pos_tag])\n",
    "    else:\n",
    "        return(word_pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "59c4c191",
   "metadata": {},
   "outputs": [],
   "source": [
    "mystem_conv_analysis_list = []\n",
    "for mystem_word_analysis in mystem_analysis_list:\n",
    "    mystem_conv_tag = convert_tags(mystem_word_analysis[1], 'mystem')\n",
    "    mystem_conv_analysis_list.append([mystem_word_analysis[0], mystem_conv_tag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "c7b4fc02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Финансовое', 'ADJ'],\n",
       " ['обеспечение', 'NOUN'],\n",
       " ['деятельности', 'NOUN'],\n",
       " ['Федерального', 'ADJ'],\n",
       " ['государственного', 'ADJ']]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystem_conv_analysis_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b2557af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "natasha_conv_analysis_list = []\n",
    "for natasha_sent_analysis in natasha_analysis.sents:\n",
    "    for natasha_word_analysis in natasha_sent_analysis.morph.tokens:\n",
    "        natasha_conv_word_analysis = [natasha_word_analysis.text,\n",
    "                                           convert_tags(natasha_word_analysis.pos, 'natasha')]\n",
    "        if natasha_conv_word_analysis[1] != None:\n",
    "            natasha_conv_analysis_list.append(natasha_conv_word_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "ea15bbf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Финансовое', 'ADJ'],\n",
       " ['обеспечение', 'NOUN'],\n",
       " ['деятельности', 'NOUN'],\n",
       " ['Федерального', 'ADJ'],\n",
       " ['государственного', 'ADJ']]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "natasha_conv_analysis_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "35d7d7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_conv_analysis_list = []\n",
    "for spacy_word_analysis in spacy_analysis_list:\n",
    "    spacy_conv_tag = convert_tags(spacy_word_analysis[1], 'spacy')\n",
    "    if spacy_conv_tag != None:\n",
    "        spacy_conv_analysis_list.append([spacy_word_analysis[0], spacy_conv_tag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "abcbf0ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Финансовое', 'ADJ'],\n",
       " ['обеспечение', 'NOUN'],\n",
       " ['деятельности', 'NOUN'],\n",
       " ['Федерального', 'ADJ'],\n",
       " ['государственного', 'ADJ'],\n",
       " ['автономного', 'ADJ'],\n",
       " ['образовательного', 'ADJ'],\n",
       " ['учреждения', 'NOUN'],\n",
       " ['высшего', 'ADJ'],\n",
       " ['профессионального', 'ADJ']]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_conv_analysis_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c23623",
   "metadata": {},
   "source": [
    "При сопоставлении самое сложное -- разобраться со съехавшими токенами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "adbeaeb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(231, 231, 232, 240)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gold_standard_list), len(mystem_conv_analysis_list), len(natasha_conv_analysis_list), len(spacy_conv_analysis_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "d3634cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv   \n",
    "    \n",
    "with open('analysis_comparison.tsv', 'w', newline='', encoding='utf-8') as file:\n",
    "    longest_analysis_len = max(len(gold_standard_list), len(mystem_conv_analysis_list),\n",
    "                               len(natasha_conv_analysis_list), len(spacy_conv_analysis_list))\n",
    "    for i in range(longest_analysis):\n",
    "        if len(gold_standard_list) <= i:\n",
    "            gold_standard_analysis = ['', '']\n",
    "        else:\n",
    "            gold_standard_analysis = gold_standard_list[i]\n",
    "        if len(mystem_conv_analysis_list) <= i:\n",
    "            mystem_analysis = ['', '']\n",
    "        else:\n",
    "            mystem_analysis = mystem_conv_analysis_list[i]\n",
    "        if len(natasha_conv_analysis_list) <= i:\n",
    "            natasha_analysis = ['', '']\n",
    "        else:\n",
    "            natasha_analysis = natasha_conv_analysis_list[i]\n",
    "        spacy_analysis = spacy_conv_analysis_list[i]\n",
    "        string = [gold_standard_analysis[0], gold_standard_analysis[1],\n",
    "                  mystem_analysis[0], mystem_analysis[1],\n",
    "                  natasha_analysis[0], natasha_analysis[1],\n",
    "                  spacy_analysis[0], spacy_analysis[1]]\n",
    "        tsv_output = csv.writer(file, delimiter='\\t')\n",
    "        tsv_output.writerow(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0816d6a",
   "metadata": {},
   "source": [
    "Далее наполовину вручную разбираемся с произошедшими из-за различных алгоритмов токенизации у разных моделей сдвигами в соответствии токенов. Если морфологизатор неверно делил слово на токены, то брался pos-тег последнего токена."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "14784128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "0f60f661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gold_standard_token</th>\n",
       "      <th>gold_standard_tag</th>\n",
       "      <th>mystem_tag</th>\n",
       "      <th>natasha_tag</th>\n",
       "      <th>spacy_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>косого</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>VERB</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>косо</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>косил</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>косую</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>косу</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    gold_standard_token gold_standard_tag mystem_tag natasha_tag spacy_tag\n",
       "226              косого               ADJ        ADJ        VERB       ADJ\n",
       "227                косо               ADV        ADV         ADV       ADV\n",
       "228               косил              VERB       VERB        VERB      VERB\n",
       "229               косую               ADJ        ADJ         ADJ      NOUN\n",
       "230                косу              NOUN       NOUN        NOUN      NOUN"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('analysis_comparison.tsv', sep='\\t', encoding='utf-8', header=None)\n",
    "df.columns = ['gold_standard_token', 'gold_standard_tag',\n",
    "              'mystem_token', 'mystem_tag',\n",
    "              'natasha_token', 'natasha_tag',\n",
    "              'spacy_token', 'spacy_tag']\n",
    "df.loc[33:,['mystem_token', 'mystem_tag']] = df.loc[33:,['mystem_token', 'mystem_tag']].shift(1)\n",
    "df.loc[35:,['mystem_token', 'mystem_tag']] = df.loc[35:,['mystem_token', 'mystem_tag']].shift(1)\n",
    "df.loc[36:,['natasha_token', 'natasha_tag', 'spacy_token', 'spacy_tag']] = \\\n",
    "                df.loc[36:,['natasha_token', 'natasha_tag', 'spacy_token', 'spacy_tag']].shift(-1)\n",
    "df.loc[89:,['mystem_token', 'mystem_tag']] = df.loc[89:,['mystem_token', 'mystem_tag']].shift(-1)\n",
    "df.loc[89:,['spacy_token', 'spacy_tag']] = df.loc[89:,['spacy_token', 'spacy_tag']].shift(-2)\n",
    "df.loc[118:,['spacy_token', 'spacy_tag']] = df.loc[118:,['spacy_token', 'spacy_tag']].shift(-1)\n",
    "df.loc[149:,['spacy_token', 'spacy_tag']] = df.loc[149:,['spacy_token', 'spacy_tag']].shift(-1)\n",
    "df.loc[150:,['spacy_token', 'spacy_tag']] = df.loc[150:,['spacy_token', 'spacy_tag']].shift(-2)\n",
    "df.loc[165:,['mystem_token', 'mystem_tag']] = df.loc[165:,['mystem_token', 'mystem_tag']].shift(-1)\n",
    "df.loc[165:,['spacy_token', 'spacy_tag']] = df.loc[165:,['spacy_token', 'spacy_tag']].shift(-2)\n",
    "df = df.loc[:230, ['gold_standard_token', 'gold_standard_tag', 'mystem_tag', 'natasha_tag', 'spacy_tag']]\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "719bfdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "d38b3c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mystem 0.8051948051948052\n",
      "Natasha 0.8398268398268398\n",
      "Spacy 0.8311688311688312\n"
     ]
    }
   ],
   "source": [
    "print('Mystem', accuracy_score(df['gold_standard_tag'], df['mystem_tag'].astype(str)))\n",
    "print('Natasha', accuracy_score(df['gold_standard_tag'], df['natasha_tag']))\n",
    "print('Spacy', accuracy_score(df['gold_standard_tag'], df['spacy_tag']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f4b893",
   "metadata": {},
   "source": [
    "Итак, из тех постеггеров, которые я сравнивала, лучшим оказался Natasha."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22385efe",
   "metadata": {},
   "source": [
    "4. (4 балла) Дальше вам нужно взять лучший теггер для русского языка и с его помощью написать функцию (chunker),  которая выделяет из размеченного текста 3 типа n-грамм, соответствующих какому-то шаблону (к примеру не + какая-то часть речи или NP или сущ.+ наречие и тд) В предыдущем дз многие из вас справедливо заметили, что если бы мы могли класть в словарь не только отдельные слова, но и словосочетания, то программа работала бы лучше. Предложите 3 шаблона (слово + POS-тег / POS-тег + POS-тег) запись которых в словарь, по вашему мнению, улучшила бы качество работы программы из предыдущей домашки. Балл за объяснение того, почему именно эти группы вы взяли, балл за создание такого рода чанкера, балл за  за встраивание функции в программу из предыдущей домашки, балл за сравнение качества предсказания тональности с улучшением и без."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cdacb0",
   "metadata": {},
   "source": [
    "## FAQ\n",
    "1. В каком формате должна быть ручная разметка корпуса? - В любом, как вам удобно (табличка токен + тег, conllu, и тд). Не забудьте загрузить его на гитхаб вместе с решением.\n",
    "2. Верно ли, что корпусом может быть набор не связанных между собой предложений? - Да.\n",
    "3. Если случай спорный, у него может быть два разбора или нужно принять единое решение? - Может быть два разбора, если так подсказывает ваша лингвистическая интуиция.\n",
    "4. Как оценивать accuracy, если POS-теггер не разрешает омонимию? - Проверить, что это точно так. За правильные ответы тогда можно считать случаи, когда тег из gold-разметки есть в ответе теггера, либо как это часто бывает, брать первый (нулевой) вариант - you decide.\n",
    "5. Что делать со штуками вроде причастия? Если теггер указывает его как глагол, а мы разметили как причастие, это стоит засчитать (потому что у теггера не было возможности узнать наш тегсет) или нет? Или как раз на моменте выбора тегсета нужно продумать его так, чтобы по возможности избежать таких ситуаций?\n",
    "\n",
    "\\- Можно при конвертации тегов посмотреть, не нужно ли учитывать для этого теггера не только POS-тег, но и какие-то другие граммемы, чтобы потом преобразовать этот тег в причастие. Собственно, для таких случаев и нужна какая-то более хитрая конвертация, а не просто словарик соответствий POS-тегов.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
